機器學習

1. 定義
不須明確的程式設計給予電腦學習的能力，讓機器找到從輸入到輸出的數學函數
輸入 + 輸出 = 函數

2. 例子
- 自然語言處理(Statistical natural language processing)
  利用語料庫以大量單字資料做到翻譯文章效果
- 圖像辨識與分類

3. 學習種類
資料 + 演算法 = 模型
- 監督式學習
  利用標籤資料告訴電腦什麼樣的資料對應什麼樣的結果
  可以解學回歸(Linear regression)和分類(Logistic regression)問題
  常見演算法: KNN(K-nearest neighbor)、SVM(Support Vector Machine)、Gradient Boostering、Neural Network
- 非監督式學習
  資料沒有給標籤，用於找到資料間的相似程度
  ex. Clustering(聚類)、降維(除掉關係不大的特徵)
  無法用於分類
- 半監督式學習
  資料有些有標籤有些則無
  部分資料有標籤可以先找出好的特徵，不僅成本效監督式低，準確率也較無監督式高
- 強化學習
  使用懲罰和獎勵邊做邊修正
  適合需要作一連串決策的動作

4. 使用機器學習時機
- 想要自動化且需要一致結果的重複決策或評估
- 難以或不可能明確描述決策背後的解決方案或準則
- 已標記資料或現有範例，可以在其中描述情況，將其對應至正確的結果

5. 監督式學習
- Linear regression: 假設資料呈現類似一條直線的線性關係
  總之就是類似數學上最適直線的概念
  步驟: 決定模型函數和誤差 -> 最小化損失
- Logistic regression: 畫出一條直線區隔兩種類型資料，只能做二元分類
  主要用sigmoid函數(邏輯函數)將任何數值轉換成0到1之間，代表發生的機率
  設定閥值，將資料二元化
  只支援二元分類
- K-Nearest Neighbor: 距離待測資料的K個資料，進行分類
  找出離測資最近的K筆資料進行分類
  以多數決的方式分到對應類別
  支援多元分類
- Gradient Descent: 主要用來做最佳化(最小化損失)
  假設一個參數w求最小損失，使用二維平面求斜率，w即多維空間的梯度
- Support Vector Machine: 邊距最大化，預測未知資料時較不會有錯誤分類的問題
  找到一個比較有餘裕的分界線，該線與每個資料點的距離最大
  離分界線最近的資料點較支持向量(Support Vector)
  使用Kernel將資料映射到更高的維度，達成線性分類
  只支援二元分類

6. 非監督式學習
- Clustering(聚類)
- K-means(K平均)
  把n個點分成k個類別，將k個重心放到任意位置
  每個資料找最近的重心，根據各個聚類的平均，更新重心的位置
  重複上個步驟
- Ward's method(沃德法): 階層式聚類
  n個點，兩兩合體，直到成為目標的聚類數(如1)

7. 強化學習
建立在馬爾可夫決策過程的架構之上
s:state  a:action  r:reward  t:time
- agent在當下的時間t觀察環境取得狀態St
- agent從狀態根據目前行為基準選擇行動At
- agent獲得報酬Rt
- 時間變成t+1反覆進行

8. 準確度
- Cross Validation(交叉驗證)
  Training set: 用來訓練的以知資料，用來訓練模型
  Test set: 未知資料，用來測試模型準確度(取所有資料的2成)
- Confusion matrix(混淆矩陣): 用一個矩陣判斷資料是否有正確分類
  x-axis: 預測標籤
  y-axis: 實際標籤

9. 名詞解釋
- epoch
  資料組完全通過演算法一次稱為一個epoch